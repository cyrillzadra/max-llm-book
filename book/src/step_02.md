# Step 02: Causal masking

<div class="note">
    Learn to create attention masks to prevent the model from "seeing" future tokens during autoregressive generation.
</div>

## Implementing causal masking

In this step you'll implement the `causal_mask()` function. This creates a [mask matrix](https://docs.modular.com/glossary/ai/attention-mask/) that prevents the model from "seeing" future tokens when predicting the next token. The mask sets attention scores to negative infinity (`-inf`) for future positions. After softmax, these `-inf` values become zero probability, blocking information flow from later tokens.

GPT-2 generates text one token at a time, left-to-right. During training, causal masking prevents the model from "cheating" by looking ahead at tokens it should be predicting. Without this mask, the model would have access to information it won't have during actual text generation.

## Understanding the mask pattern

The mask creates a lower triangular pattern where each token can only attend to itself and previous tokens:

- Position 0 attends to: position 0 only
- Position 1 attends to: positions 0-1
- Position 2 attends to: positions 0-2
- And so on...

The mask shape is `(sequence_length, sequence_length + num_tokens)`. This allows attending to both the current sequence and previous context from the KV cache.

<div class="note">
<div class="title">MAX operations</div>

You'll use the following MAX operations to complete this task:

**Functional decorator**:
- [`@F.functional`](https://docs.modular.com/max/api/python/experimental/functional/#max.experimental.functional.functional): Converts functions to graph operations for MAX compilation

**Tensor operations**:
- [`Tensor.constant()`](https://docs.modular.com/max/api/python/experimental/tensor#max.experimental.tensor.Tensor.constant): Creates a scalar constant tensor
- [`F.broadcast_to()`](https://docs.modular.com/max/api/python/experimental/functional#max.experimental.functional.broadcast_to): Expands tensor dimensions to target shape
- [`F.band_part()`](https://docs.modular.com/max/api/python/experimental/functional#max.experimental.functional.band_part): Extracts band matrix (keeps diagonal band, zeros out rest)

</div>

## Implementing the mask

You'll create the causal mask in several steps:

1. **Import required modules**: Import `Device`, `DType`, `functional as F`, `Tensor`, `Dim`, and `DimLike` from MAX libraries.

2. **Add @F.functional decorator**: This converts the function to a MAX graph operation.

3. **Calculate total sequence length**: Combine `sequence_length` and `num_tokens` using `Dim()` to determine mask width.

4. **Create constant tensor**: Use `Tensor.constant(float("-inf"), dtype=dtype, device=device)` to create a scalar that will be broadcast.

5. **Broadcast to target shape**: Use `F.broadcast_to(mask, shape=(sequence_length, n))` to expand the scalar to a 2D matrix.

6. **Apply band part**: Use `F.band_part(mask, num_lower=None, num_upper=0, exclude=True)` to create the lower triangular pattern. This keeps 0s on and below the diagonal, `-inf` above.

**Implementation** (`step_02.py`):

```python
{{#include ../../steps/step_02.py}}
```

### Validation

Run `pixi run s02` to verify your implementation.

**Reference**: `solutions/solution_02.py`

**Next**: In [Step 03](./step_03.md), you'll implement layer normalization to stabilize activations for effective training.
